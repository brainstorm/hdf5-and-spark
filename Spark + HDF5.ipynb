{
 "metadata": {
  "name": "",
  "signature": "sha256:cf3000136ec2c84574ae44560df4f117fad3c70bd7dd4e4f6876297b9a3500d6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example glue code for Spark+HDF5\n",
      "--------------------------------\n",
      "Writing a generic Hadoop input format for reading HDF5 into Spark will be hard, but we can try using `sc.parallelize` alongside Python's HDF5 libraries to manually load the contents of an HDF5 container in parallel. This notebooks demonstrates basic working functionality, at least for HDF5 files with simple array data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set a path to the data (customize accordingly)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = '/Users/freemanj11/Desktop/hdf5testing/hdf5/'\n",
      "filename = 'testfile12.hdf5'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import h5py as h5\n",
      "from numpy import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create an HDF5 file and populate with random data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = h5.File(path + filename,'w')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f.create_dataset('mydata', (100,1000))\n",
      "f['/mydata'][:] = random.randn(100,1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the file and show the contents locally"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = h5.File(path + filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file['/mydata'][:].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "(100, 1000)"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try to load the contents of the contained array in parallel via Spark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def readchunk(v):\n",
      "    file = h5.File(path + filename)\n",
      "    return file['/mydata'][v,:]\n",
      "\n",
      "foo = sc.parallelize(range(0,100)).map(lambda v: readchunk(v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do a count... if this works, the loading ran without error (but still need to check contents)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 86,
       "text": [
        "100"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the first record, should be an (1000,) array"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo.first().shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "(1000,)"
       ]
      }
     ],
     "prompt_number": 87
    }
   ],
   "metadata": {}
  }
 ]
}